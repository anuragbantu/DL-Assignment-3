{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11812889,"sourceType":"datasetVersion","datasetId":7419453}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:20.803332Z","iopub.execute_input":"2025-05-20T09:27:20.803557Z","iopub.status.idle":"2025-05-20T09:27:22.334602Z","shell.execute_reply.started":"2025-05-20T09:27:20.803538Z","shell.execute_reply":"2025-05-20T09:27:22.333745Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/telugu-lexicon/te.translit.sampled.train.tsv\n/kaggle/input/telugu-lexicon/te.translit.sampled.dev.tsv\n/kaggle/input/telugu-lexicon/te.translit.sampled.test.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1. Data Import\n","metadata":{}},{"cell_type":"markdown","source":"Import the data from the TSV files.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load train and validation datasets from TSV files\ntrain_df = pd.read_csv(\"/kaggle/input/telugu-lexicon/te.translit.sampled.train.tsv\", sep=\"\\t\", header=None)\ntrain_df.columns = [\"target\", \"source\", \"label\"]\n\n\nval_df = pd.read_csv(\"/kaggle/input/telugu-lexicon/te.translit.sampled.dev.tsv\", sep=\"\\t\", header=None)\nval_df.columns = [\"target\", \"source\", \"label\"]\n\n# Drop any rows where source or target is missing\ntrain_df = train_df.dropna(subset=[\"source\", \"target\"])\nval_df = val_df.dropna(subset=[\"source\", \"target\"])\n\n# Ensure source and target are strings\ntrain_df[\"source\"] = train_df[\"source\"].astype(str)\ntrain_df[\"target\"] = train_df[\"target\"].astype(str)\nval_df[\"source\"] = val_df[\"source\"].astype(str)\nval_df[\"target\"] = val_df[\"target\"].astype(str)\n\n# Create pairs for training and validation\npairs = list(zip(train_df[\"source\"], train_df[\"target\"]))\nval_pairs = list(zip(val_df[\"source\"], val_df[\"target\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:22.335324Z","iopub.execute_input":"2025-05-20T09:27:22.335775Z","iopub.status.idle":"2025-05-20T09:27:22.633895Z","shell.execute_reply.started":"2025-05-20T09:27:22.335743Z","shell.execute_reply":"2025-05-20T09:27:22.633101Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import random\n\n# Sample 10,000 examples for faster experimentation. Otherwise, training takes too long\n\nsample_size = 10000\npairs = random.sample(pairs, sample_size)    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:22.635419Z","iopub.execute_input":"2025-05-20T09:27:22.635650Z","iopub.status.idle":"2025-05-20T09:27:22.646091Z","shell.execute_reply.started":"2025-05-20T09:27:22.635632Z","shell.execute_reply":"2025-05-20T09:27:22.644869Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:22.647572Z","iopub.execute_input":"2025-05-20T09:27:22.647773Z","iopub.status.idle":"2025-05-20T09:27:29.685924Z","shell.execute_reply.started":"2025-05-20T09:27:22.647757Z","shell.execute_reply":"2025-05-20T09:27:29.685336Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 2. Tokenization and Vocabulary Indexing\n","metadata":{}},{"cell_type":"code","source":"# Extract unique characters for both source and target languages\n\nSRC_CHARS = set(\"\".join(s for s, _ in pairs))\nTRG_CHARS = set(\"\".join(t for _, t in pairs)) | {\"<sos>\", \"<eos>\"}# include special tokens\n\n# Create index mappings for characters\n\nsrc2idx = {ch: i+1 for i, ch in enumerate(sorted(SRC_CHARS))}  # reserve 0 for padding\nsrc2idx[\"<pad>\"] = 0\ntrg2idx = {ch: i+1 for i, ch in enumerate(sorted(TRG_CHARS))}\ntrg2idx[\"<pad>\"] = 0\n\nidx2trg = {i: ch for ch, i in trg2idx.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:29.686723Z","iopub.execute_input":"2025-05-20T09:27:29.687110Z","iopub.status.idle":"2025-05-20T09:27:29.703595Z","shell.execute_reply.started":"2025-05-20T09:27:29.687085Z","shell.execute_reply":"2025-05-20T09:27:29.703049Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# 3. Model Definition: Encoder-Decoder\n\nDefine the encoder-decoder structure.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Utility function to select the RNN cell type\ndef get_rnn_cell(cell_type):\n    cell_type = cell_type.upper()\n    if cell_type == \"GRU\":\n        return nn.GRU\n    elif cell_type == \"LSTM\":\n        return nn.LSTM\n    elif cell_type == \"RNN\":\n        return nn.RNN\n    else:\n        raise ValueError(\"Unsupported RNN cell type. Use 'RNN', 'GRU', or 'LSTM'.\")\n\n# Encoder: Converts input sequences to hidden representations\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, cell_type=\"GRU\", num_layers=1, dropout=0.0):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n        self.rnn = get_rnn_cell(cell_type)(\n            emb_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0.0\n        )\n    def forward(self, src):\n        embedded = self.embedding(src)  # [src_len, batch=1, emb_dim]\n        outputs, hidden = self.rnn(embedded)  # hidden: [num_layers, batch, hidden_dim]\n        return hidden\n\n# Decoder: Generates output sequences one token at a time\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, cell_type=\"GRU\", num_layers=1, dropout=0.0):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n        self.rnn = get_rnn_cell(cell_type)(\n            emb_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.out = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_step, hidden):\n        embedded = self.embedding(input_step)  \n        output, hidden = self.rnn(embedded, hidden)\n        output = self.dropout(output.squeeze(0))  # Apply dropout to RNN output\n        prediction = self.out(output)             \n        return prediction, hidden\n\n# Seq2Seq: Full encoder-decoder wrapper\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device, sos_idx):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        self.sos_idx = sos_idx\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        trg_len = trg.shape[0]\n        output_dim = self.decoder.out.out_features\n        outputs = torch.zeros(trg_len, 1, output_dim).to(self.device)\n\n        hidden = self.encoder(src)\n        hidden = self.adjust_hidden_for_decoder(hidden, self.decoder.rnn.num_layers)\n\n        input_step = torch.tensor([[self.sos_idx]], device=self.device) # Start with <sos>\n\n        for t in range(trg_len):\n            output, hidden = self.decoder(input_step, hidden)\n            outputs[t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1).unsqueeze(0) # Get top predicted token\n            input_step = trg[t].unsqueeze(0) if teacher_force else top1\n\n        return outputs\n\n    def adjust_hidden_for_decoder(self, hidden, target_layers):\n        \"\"\"\n        Adjust the encoder's hidden state to match the number of decoder layers.\n        Pads if encoder has fewer layers, trims if more.\n        Works for GRU/RNN (tensor) and LSTM (tuple).\n        \"\"\"\n        if isinstance(hidden, tuple):  # LSTM\n            h, c = hidden\n            h = self._match_layers(h, target_layers)\n            c = self._match_layers(c, target_layers)\n            return (h, c)\n        else:  # GRU or RNN\n            return self._match_layers(hidden, target_layers)\n\n    def _match_layers(self, state, target_layers):\n        \"\"\"\n        Pad or trim the hidden state tensor to match target number of layers.\n        \"\"\"\n        current_layers = state.size(0)\n        if current_layers == target_layers:\n            return state\n        elif current_layers < target_layers:\n            diff = target_layers - current_layers\n            pad = torch.zeros(diff, state.size(1), state.size(2), device=state.device)\n            return torch.cat([state, pad], dim=0)\n        else:  # current_layers > target_layers\n            return state[:target_layers]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:29.704522Z","iopub.execute_input":"2025-05-20T09:27:29.704757Z","iopub.status.idle":"2025-05-20T09:27:29.726591Z","shell.execute_reply.started":"2025-05-20T09:27:29.704741Z","shell.execute_reply":"2025-05-20T09:27:29.726081Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#use GPU for processing if available\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:29.727227Z","iopub.execute_input":"2025-05-20T09:27:29.727522Z","iopub.status.idle":"2025-05-20T09:27:29.830602Z","shell.execute_reply.started":"2025-05-20T09:27:29.727506Z","shell.execute_reply":"2025-05-20T09:27:29.830038Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Helper function that converts a word into a tensor of indices based on a given vocabulary\n# Optionally adds the <eos> token at the end\n\ndef tensor_from_word(word, mapping, add_eos=False):\n    indices = [mapping[ch] for ch in word]\n    if add_eos:\n        indices.append(mapping[\"<eos>\"])\n    return torch.tensor(indices, dtype=torch.long, device=DEVICE).unsqueeze(1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:29.831348Z","iopub.execute_input":"2025-05-20T09:27:29.831614Z","iopub.status.idle":"2025-05-20T09:27:29.847279Z","shell.execute_reply.started":"2025-05-20T09:27:29.831596Z","shell.execute_reply":"2025-05-20T09:27:29.846666Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"#  4. Beam Search Decoding \n\nDefine the beam search function.","metadata":{}},{"cell_type":"code","source":"# Decodes a sequence using beam search to find the most probable output string\n\ndef beam_search(model, src_tensor, beam_size=3, max_len=30):\n    model.eval()\n    with torch.no_grad():\n        # Encode the input sequence\n        hidden = model.encoder(src_tensor)\n        hidden = model.adjust_hidden_for_decoder(hidden, model.decoder.rnn.num_layers)\n\n\n        # Trim LSTM hidden states if necessary\n        if isinstance(hidden, tuple):\n            h, c = hidden\n            hidden = (\n                h[:model.decoder.rnn.num_layers],\n                c[:model.decoder.rnn.num_layers],\n            )\n        else:\n            hidden = hidden[:model.decoder.rnn.num_layers]\n\n        # Initialize beam with sequence starting with <sos> token\n        sequences = [([model.sos_idx], 0.0, hidden)]\n\n        for _ in range(max_len):\n            all_candidates = []\n            for seq, score, h in sequences:\n                input_step = torch.tensor([[seq[-1]]], device=model.device)\n                output, h_new = model.decoder(input_step, h) # Run decoder\n                probs = torch.log_softmax(output, dim=1) # Log-probabilities\n                topk = torch.topk(probs, beam_size) # Top-k predictions\n\n                # Expand each current sequence with top-k new tokens\n                for i in range(beam_size):\n                    token = topk.indices[0][i].item()\n                    token_score = topk.values[0][i].item()\n                    new_seq = seq + [token]\n                    all_candidates.append((new_seq, score + token_score, h_new))\n\n            # Keep top beam_size sequences with highest scores\n            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n\n            # If all sequences have ended with <eos>, stop early\n            if all(seq[-1] == trg2idx[\"<eos>\"] for seq, _, _ in sequences):\n                break\n\n        # Return best sequence (excluding <sos> and <eos> tokens)\n        best_seq = sequences[0][0][1:]  # remove <sos>\n        return \"\".join([idx2trg[i] for i in best_seq if i != trg2idx[\"<eos>\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:29.849498Z","iopub.execute_input":"2025-05-20T09:27:29.849678Z","iopub.status.idle":"2025-05-20T09:27:29.864714Z","shell.execute_reply.started":"2025-05-20T09:27:29.849658Z","shell.execute_reply":"2025-05-20T09:27:29.864206Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"#  5. Training and Evaluation Utilities \n\nDefine the training function and run the wandb sweep.","metadata":{}},{"cell_type":"code","source":"#install wandb and login\n\n!pip install -q wandb\nimport wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:29.865467Z","iopub.execute_input":"2025-05-20T09:27:29.866115Z","iopub.status.idle":"2025-05-20T09:27:38.163372Z","shell.execute_reply.started":"2025-05-20T09:27:29.866094Z","shell.execute_reply":"2025-05-20T09:27:38.162745Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"wandb.login(key='af7d7cf29d8954a13afb06c7a0d0c196c36ac51b')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:38.164103Z","iopub.execute_input":"2025-05-20T09:27:38.164514Z","iopub.status.idle":"2025-05-20T09:27:44.396926Z","shell.execute_reply.started":"2025-05-20T09:27:38.164491Z","shell.execute_reply":"2025-05-20T09:27:44.396387Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma24m003\u001b[0m (\u001b[33mma24m003-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"#set all the hyperparameter values to test in the sweep\n\nsweep_config = {\n    \"method\": \"bayes\",\n    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"emb_dim\": {\"values\": [16,32,64,256]},\n        \"hidden_dim\": {\"values\": [16,32,64,256]},\n        \"cell_type\": {\"values\": [\"RNN\",\"GRU\",\"LSTM\"]},\n        \"enc_layers\": {\"values\": [1,2,3]},\n        \"dec_layers\": {\"values\": [1, 2, 3]},\n        \"dropout\": {\"values\": [0,0.2, 0.3]},\n        \"beam_size\": {\"values\": [1, 3, 2]},\n        \"lr\": {\"values\": [0.001, 0.0005]},\n        \"teacher_forcing_ratio\": {\"values\": [0.3, 0.5, 0.7, 1.0]}\n\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:44.397662Z","iopub.execute_input":"2025-05-20T09:27:44.398125Z","iopub.status.idle":"2025-05-20T09:27:44.402583Z","shell.execute_reply.started":"2025-05-20T09:27:44.398098Z","shell.execute_reply":"2025-05-20T09:27:44.402002Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# This section includes the model training loop, validation metrics computation,\n# and logging to Weights & Biases for hyperparameter tuning and experiment tracking.\n\n\ndef train_model(config=None):\n    # Initialize a Weights & Biases run with the given configuration\n    with wandb.init(config=config) as run:\n        config = wandb.config\n\n        # Set a descriptive run name for easier tracking in WandB dashboard\n        run.name = f\"emb{config.emb_dim}_hid{config.hidden_dim}_{config.cell_type}_enc{config.enc_layers}_dec{config.dec_layers}_drop{int(config.dropout*100)}_beam{config.beam_size}_lr{config.lr}\"\n\n        # Initialize encoder and decoder models\n        encoder = Encoder(len(src2idx), config.emb_dim, config.hidden_dim,\n                          config.cell_type, config.enc_layers, config.dropout)\n        decoder = Decoder(len(trg2idx), config.emb_dim, config.hidden_dim,\n                          config.cell_type, config.dec_layers, config.dropout)\n        model = Seq2Seq(encoder, decoder, DEVICE, sos_idx=trg2idx[\"<sos>\"]).to(DEVICE)\n\n        # Define optimizer and loss criterion\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n        criterion = nn.CrossEntropyLoss(ignore_index=trg2idx[\"<pad>\"])\n        beam_size = config.get(\"beam_size\", 3)\n\n        # Training loop across multiple epochs\n        for epoch in range(1, 6):\n            model.train()\n            total_loss = 0\n            train_correct = 0\n            train_total = 0\n\n            # === Training Phase ===\n            for src_word, trg_word in pairs:\n                # Convert source and target words to tensors\n                src_tensor = tensor_from_word(src_word, src2idx)\n                trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)\n\n                # Zero the parameter gradients\n                optimizer.zero_grad()\n\n                # Forward pass with optional teacher forcing\n                output = model(src_tensor, trg_tensor, teacher_forcing_ratio=config.get(\"teacher_forcing_ratio\", 0.5))\n                loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))\n\n                # Backpropagation and parameter update\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n                # Character-level training accuracy\n                pred_tokens = output.argmax(-1).view(-1)\n                true_tokens = trg_tensor.view(-1)\n                mask = true_tokens != trg2idx[\"<pad>\"]\n                correct = (pred_tokens == true_tokens) & mask\n                train_correct += correct.sum().item()\n                train_total += mask.sum().item()\n\n            # === Validation Phase ===\n            model.eval()\n            val_loss = 0\n            val_correct = 0\n            val_total = 0\n            exact_match_count = 0\n\n            with torch.no_grad():\n                for src_word, trg_word in val_pairs:\n                    src_tensor = tensor_from_word(src_word, src2idx)\n                    trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)\n\n                    # Predict without teacher forcing\n                    output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0.0)\n                    loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))\n                    val_loss += loss.item()\n\n                    # Character-level validation accuracy\n                    pred_tokens = output.argmax(-1).view(-1)\n                    true_tokens = trg_tensor.view(-1)\n                    mask = true_tokens != trg2idx[\"<pad>\"]\n                    correct = (pred_tokens == true_tokens) & mask\n                    val_correct += correct.sum().item()\n                    val_total += mask.sum().item()\n\n                    # Sequence-level exact match using beam search\n                    pred_str = beam_search(model, src_tensor, beam_size=beam_size)\n                    if pred_str == trg_word:\n                        exact_match_count += 1\n\n            # Compute metrics\n            train_acc = train_correct / train_total\n            val_acc = val_correct / val_total\n            exact_match = exact_match_count / len(val_pairs)\n            \n            # Print metrics for the current epoch\n            print(f\"Epoch {epoch:2d} | \"\n                  f\"Train Loss: {total_loss / len(pairs):.4f} | \"\n                  f\"Train Acc: {train_acc:.4f} | \"\n                  f\"Val Loss: {val_loss / len(val_pairs):.4f} | \"\n                  f\"Val Acc: {val_acc:.4f} | \"\n                  f\"Val Exact Match: {exact_match:.4f} | \"\n                  f\"Beam Size: {beam_size}\")\n\n            \n            # Log metrics to WandB for visualization and tracking\n            wandb.log({\n                \"epoch\": epoch,\n                \"train_loss\": total_loss / len(pairs),\n                \"val_loss\": val_loss / len(val_pairs),\n                \"train_accuracy\": train_correct / train_total,\n                \"val_accuracy\": val_correct / val_total,\n                \"val_exact_match\": exact_match_count / len(val_pairs),\n                \"beam_size\": beam_size\n            })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:44.403369Z","iopub.execute_input":"2025-05-20T09:27:44.403543Z","iopub.status.idle":"2025-05-20T09:27:44.428868Z","shell.execute_reply.started":"2025-05-20T09:27:44.403529Z","shell.execute_reply":"2025-05-20T09:27:44.428382Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#run the wandb sweep\n\n\nsweep_id = wandb.sweep(sweep_config, project=\"transliteration-sweep\")\nwandb.agent(sweep_id, function=train_model, count=10)\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:44.429573Z","iopub.execute_input":"2025-05-20T09:27:44.430015Z","iopub.status.idle":"2025-05-20T09:27:44.449547Z","shell.execute_reply.started":"2025-05-20T09:27:44.429993Z","shell.execute_reply":"2025-05-20T09:27:44.448921Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'\\nsweep_id = wandb.sweep(sweep_config, project=\"transliteration-sweep\")\\nwandb.agent(sweep_id, function=train_model, count=2)\\nwandb.finish()\\n'"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# 6. Best model Evaluation","metadata":{}},{"cell_type":"markdown","source":"Evaluate the best performing model on the test dataset. The following configuration was observed to be the best performing model from the wandb sweep analysis.","metadata":{}},{"cell_type":"code","source":"# Define the best configuration obtained from hyperparameter tuning\n\nbest_config = {\n    \"emb_dim\": 32,\n    \"hidden_dim\": 256,\n    \"cell_type\": \"LSTM\",\n    \"enc_layers\": 3,\n    \"dec_layers\": 2,\n    \"dropout\": 0,\n    \"beam_size\": 3,\n    \"lr\": 0.001}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:44.450148Z","iopub.execute_input":"2025-05-20T09:27:44.450327Z","iopub.status.idle":"2025-05-20T09:27:44.464725Z","shell.execute_reply.started":"2025-05-20T09:27:44.450313Z","shell.execute_reply":"2025-05-20T09:27:44.464082Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Defining a new train function (with similar structure as the training function before) which returns the best model (with trained weights) as output so that we can test the performance on test data. ","metadata":{}},{"cell_type":"code","source":"# Train the model using the best configuration\n\ndef best_train_model(config=None):\n    with wandb.init(config=config):\n        config = wandb.config\n\n        encoder = Encoder(len(src2idx), config.emb_dim, config.hidden_dim,\n                          config.cell_type, config.enc_layers, config.dropout)\n        decoder = Decoder(len(trg2idx), config.emb_dim, config.hidden_dim,\n                          config.cell_type, config.dec_layers, config.dropout)\n        model = Seq2Seq(encoder, decoder, DEVICE, sos_idx=trg2idx[\"<sos>\"]).to(DEVICE)\n\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n        criterion = nn.CrossEntropyLoss(ignore_index=trg2idx[\"<pad>\"])\n        beam_size = config.get(\"beam_size\", 3)\n\n        #train the best model for 10 epochs\n        for epoch in range(1, 11):\n            model.train()\n            total_loss = 0\n            train_correct = 0\n            train_total = 0\n\n            for src_word, trg_word in pairs:\n                src_tensor = tensor_from_word(src_word, src2idx)\n                trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)\n\n                optimizer.zero_grad()\n                output = model(src_tensor, trg_tensor, teacher_forcing_ratio=config.get(\"teacher_forcing_ratio\", 0.5))\n                loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n                pred_tokens = output.argmax(-1).view(-1)\n                true_tokens = trg_tensor.view(-1)\n                mask = true_tokens != trg2idx[\"<pad>\"]\n                correct = (pred_tokens == true_tokens) & mask\n                train_correct += correct.sum().item()\n                train_total += mask.sum().item()\n\n            model.eval()\n            val_loss = 0\n            val_correct = 0\n            val_total = 0\n            exact_match_count = 0\n\n            with torch.no_grad():\n                for src_word, trg_word in val_pairs:\n                    src_tensor = tensor_from_word(src_word, src2idx)\n                    trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)\n\n                    output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0.0)\n                    loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))\n                    val_loss += loss.item()\n\n                    pred_tokens = output.argmax(-1).view(-1)\n                    true_tokens = trg_tensor.view(-1)\n                    mask = true_tokens != trg2idx[\"<pad>\"]\n                    correct = (pred_tokens == true_tokens) & mask\n                    val_correct += correct.sum().item()\n                    val_total += mask.sum().item()\n\n                    # Beam search for exact match accuracy\n                    pred_str = beam_search(model, src_tensor, beam_size=beam_size)\n                    if pred_str == trg_word:\n                        exact_match_count += 1\n\n            train_acc = train_correct / train_total\n            val_acc = val_correct / val_total\n            exact_match = exact_match_count / len(val_pairs)\n\n            print(f\"Epoch {epoch:2d} | \"\n                  f\"Train Loss: {total_loss / len(pairs):.4f} | \"\n                  f\"Train Acc: {train_acc:.4f} | \"\n                  f\"Val Loss: {val_loss / len(val_pairs):.4f} | \"\n                  f\"Val Acc: {val_acc:.4f} | \"\n                  f\"Val Exact Match: {exact_match:.4f} | \"\n                  f\"Beam Size: {beam_size}\")\n\n            wandb.log({\n                \"epoch\": epoch,\n                \"train_loss\": total_loss / len(pairs),\n                \"val_loss\": val_loss / len(val_pairs),\n                \"train_accuracy\": train_acc,\n                \"val_accuracy\": val_acc,\n                \"val_exact_match\": exact_match,\n                \"beam_size\": beam_size\n            })\n\n        return model  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:44.465588Z","iopub.execute_input":"2025-05-20T09:27:44.466300Z","iopub.status.idle":"2025-05-20T09:27:44.488860Z","shell.execute_reply.started":"2025-05-20T09:27:44.466277Z","shell.execute_reply":"2025-05-20T09:27:44.488392Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#train the best model\n\nmodel = best_train_model(config=best_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:27:44.489534Z","iopub.execute_input":"2025-05-20T09:27:44.490053Z","iopub.status.idle":"2025-05-20T10:15:28.172609Z","shell.execute_reply.started":"2025-05-20T09:27:44.490030Z","shell.execute_reply":"2025-05-20T10:15:28.171958Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_092744-fiyb613z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m003-iit-madras/uncategorized/runs/fiyb613z' target=\"_blank\">lyric-water-47</a></strong> to <a href='https://wandb.ai/ma24m003-iit-madras/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m003-iit-madras/uncategorized' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m003-iit-madras/uncategorized/runs/fiyb613z' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/uncategorized/runs/fiyb613z</a>"},"metadata":{}},{"name":"stdout","text":"Epoch  1 | Train Loss: 1.8489 | Train Acc: 0.4849 | Val Loss: 1.3592 | Val Acc: 0.5868 | Val Exact Match: 0.1177 | Beam Size: 3\nEpoch  2 | Train Loss: 0.7951 | Train Acc: 0.7584 | Val Loss: 1.0860 | Val Acc: 0.6849 | Val Exact Match: 0.2245 | Beam Size: 3\nEpoch  3 | Train Loss: 0.5327 | Train Acc: 0.8376 | Val Loss: 1.0779 | Val Acc: 0.7117 | Val Exact Match: 0.2634 | Beam Size: 3\nEpoch  4 | Train Loss: 0.3924 | Train Acc: 0.8780 | Val Loss: 1.1085 | Val Acc: 0.7183 | Val Exact Match: 0.2763 | Beam Size: 3\nEpoch  5 | Train Loss: 0.3069 | Train Acc: 0.9053 | Val Loss: 1.1522 | Val Acc: 0.7305 | Val Exact Match: 0.3083 | Beam Size: 3\nEpoch  6 | Train Loss: 0.2437 | Train Acc: 0.9232 | Val Loss: 1.1801 | Val Acc: 0.7334 | Val Exact Match: 0.3519 | Beam Size: 3\nEpoch  7 | Train Loss: 0.2132 | Train Acc: 0.9345 | Val Loss: 1.2433 | Val Acc: 0.7268 | Val Exact Match: 0.3152 | Beam Size: 3\nEpoch  8 | Train Loss: 0.1925 | Train Acc: 0.9407 | Val Loss: 1.2433 | Val Acc: 0.7375 | Val Exact Match: 0.3134 | Beam Size: 3\nEpoch  9 | Train Loss: 0.1729 | Train Acc: 0.9457 | Val Loss: 1.3180 | Val Acc: 0.7320 | Val Exact Match: 0.2639 | Beam Size: 3\nEpoch 10 | Train Loss: 0.1588 | Train Acc: 0.9502 | Val Loss: 1.3442 | Val Acc: 0.7335 | Val Exact Match: 0.3335 | Beam Size: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beam_size</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▇▇██████</td></tr><tr><td>val_exact_match</td><td>▁▄▅▆▇█▇▇▅▇</td></tr><tr><td>val_loss</td><td>█▁▁▂▃▄▅▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beam_size</td><td>3</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.95023</td></tr><tr><td>train_loss</td><td>0.1588</td></tr><tr><td>val_accuracy</td><td>0.73353</td></tr><tr><td>val_exact_match</td><td>0.33345</td></tr><tr><td>val_loss</td><td>1.34416</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lyric-water-47</strong> at: <a href='https://wandb.ai/ma24m003-iit-madras/uncategorized/runs/fiyb613z' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/uncategorized/runs/fiyb613z</a><br> View project at: <a href='https://wandb.ai/ma24m003-iit-madras/uncategorized' target=\"_blank\">https://wandb.ai/ma24m003-iit-madras/uncategorized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_092744-fiyb613z/logs</code>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"#load the test data\ntest_df = pd.read_csv(\"/kaggle/input/telugu-lexicon/te.translit.sampled.test.tsv\", sep=\"\\t\", header=None)\ntest_df.columns = [\"target\", \"source\", \"label\"]  # adjust if only 2 columns\ntest_df = test_df.dropna(subset=[\"source\", \"target\"])\ntest_df[\"source\"] = test_df[\"source\"].astype(str)\ntest_df[\"target\"] = test_df[\"target\"].astype(str)\ntest_pairs = list(zip(test_df[\"source\"], test_df[\"target\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:15:28.173389Z","iopub.execute_input":"2025-05-20T10:15:28.173901Z","iopub.status.idle":"2025-05-20T10:15:28.210574Z","shell.execute_reply.started":"2025-05-20T10:15:28.173883Z","shell.execute_reply":"2025-05-20T10:15:28.209872Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(len(test_pairs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:15:28.211299Z","iopub.execute_input":"2025-05-20T10:15:28.211714Z","iopub.status.idle":"2025-05-20T10:15:28.230153Z","shell.execute_reply.started":"2025-05-20T10:15:28.211696Z","shell.execute_reply":"2025-05-20T10:15:28.229519Z"}},"outputs":[{"name":"stdout","text":"5747\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Evaluate model performance on test set\n\nmodel.eval()\ntest_correct = 0\ntest_total = 0\nexact_match_count = 0\nbeam_size = best_config[\"beam_size\"]\n\nwith torch.no_grad():\n    for src_word, trg_word in test_pairs:\n        src_tensor = tensor_from_word(src_word, src2idx)\n        trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)\n\n        output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0.0)\n\n        # Character-level accuracy\n        pred_tokens = output.argmax(-1).view(-1)\n        true_tokens = trg_tensor.view(-1)\n        mask = true_tokens != trg2idx[\"<pad>\"]\n        correct = (pred_tokens == true_tokens) & mask\n        test_correct += correct.sum().item()\n        test_total += mask.sum().item()\n\n        # Word-level exact match via beam search\n        pred_str = beam_search(model, src_tensor, beam_size=beam_size)\n        if pred_str == trg_word:\n            exact_match_count += 1\n\ntest_char_acc = test_correct / test_total\ntest_exact_match = exact_match_count / len(test_pairs)\n\nprint(f\"Test Char Accuracy: {test_char_acc:.4f}\")\nprint(f\"Test Exact Match:  {test_exact_match:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:15:28.230788Z","iopub.execute_input":"2025-05-20T10:15:28.230961Z","iopub.status.idle":"2025-05-20T10:17:20.311396Z","shell.execute_reply.started":"2025-05-20T10:15:28.230947Z","shell.execute_reply":"2025-05-20T10:17:20.310590Z"}},"outputs":[{"name":"stdout","text":"Test Char Accuracy: 0.7402\nTest Exact Match:  0.3348\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"Saving all the prediction on test data to a TSV file in folder predictions_vanilla.","metadata":{}},{"cell_type":"code","source":"import csv, os, pandas as pd, torch\n\n#  Run model on the whole test set once \nmodel.eval()\nbeam = best_config[\"beam_size\"]\nrecords = []                         # rows for the output file\n\nwith torch.no_grad():\n    for latin, true in test_pairs:\n        pred = beam_search(model,\n                           tensor_from_word(latin, src2idx),\n                           beam_size=beam)\n        records.append({\"latin\": latin,\n                        \"true\": true,\n                        \"pred\": pred})\n\n# Save as TSV \nout_df = pd.DataFrame(records)\nos.makedirs(\"predictions_vanilla\", exist_ok=True)          # local folder\nout_path = \"predictions_vanilla/test_predictions.tsv\"\nout_df.to_csv(out_path, sep=\"\\t\", index=False)\nprint(f\"Saved {len(out_df)} rows ➜ {out_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:17:20.312284Z","iopub.execute_input":"2025-05-20T10:17:20.312549Z","iopub.status.idle":"2025-05-20T10:18:47.384973Z","shell.execute_reply.started":"2025-05-20T10:17:20.312533Z","shell.execute_reply":"2025-05-20T10:18:47.384346Z"}},"outputs":[{"name":"stdout","text":"Saved 5747 rows ➜ predictions_vanilla/test_predictions.tsv\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Visualizing the predictions of the best model.","metadata":{}},{"cell_type":"code","source":"# Display random predictions in a color-coded table\n\nfrom tabulate import tabulate\nimport random\n\ndef display_predictions(model, test_pairs, src2idx, beam_size=3, max_samples=10):\n    model.eval()\n    table = []\n\n    # ANSI escape codes for red and green coloring in terminal output\n    RED = \"\\033[91m\"\n    GREEN = \"\\033[92m\"\n    RESET = \"\\033[0m\"\n\n    col_widths = {\n        \"input\": 18,\n        \"pred\": 25,\n        \"target\": 25,\n        \"match\": 7\n    }\n\n    with torch.no_grad():\n        # Randomly sample a few test examples\n        sampled_pairs = random.sample(test_pairs, k=min(max_samples, len(test_pairs)))\n        for src_word, trg_word in sampled_pairs:\n            # Convert input to tensor and run beam search\n            src_tensor = tensor_from_word(src_word, src2idx)\n            pred_str = beam_search(model, src_tensor, beam_size=beam_size)\n            is_match = pred_str == trg_word\n\n            # Format each column entry with appropriate alignment\n            row_data = [\n                f\"{src_word:<{col_widths['input']}}\",\n                f\"{pred_str:<{col_widths['pred']}}\",\n                f\"{trg_word:<{col_widths['target']}}\",\n                \"✓\" if is_match else \"✗\"\n            ]\n\n            # Apply color to entire row based on correctness\n            color = GREEN if is_match else RED\n            colored_row = [f\"{color}{cell}{RESET}\" for cell in row_data]\n\n            table.append(colored_row)\n\n    # Create headers with center alignment\n    headers = [\n        f\"{'Input (Latin)':^{col_widths['input']}}\",\n        f\"{'Predicted (Telugu)':^{col_widths['pred']}}\",\n        f\"{'Target (Telugu)':^{col_widths['target']}}\",\n        f\"{'Match':^{col_widths['match']}}\"\n    ]\n\n    # Display the results as a formatted table\n    print(tabulate(table, headers=headers, tablefmt=\"fancy_grid\", stralign=\"center\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:18:47.385711Z","iopub.execute_input":"2025-05-20T10:18:47.385966Z","iopub.status.idle":"2025-05-20T10:18:47.418450Z","shell.execute_reply.started":"2025-05-20T10:18:47.385940Z","shell.execute_reply":"2025-05-20T10:18:47.417937Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"display_predictions(model, test_pairs,src2idx, beam_size=best_config[\"beam_size\"], max_samples=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:18:47.419039Z","iopub.execute_input":"2025-05-20T10:18:47.419232Z","iopub.status.idle":"2025-05-20T10:18:47.580105Z","shell.execute_reply.started":"2025-05-20T10:18:47.419217Z","shell.execute_reply":"2025-05-20T10:18:47.579396Z"}},"outputs":[{"name":"stdout","text":"╒══════════════════════╤═════════════════════════════╤═════════════════════════════╤═══════════╕\n│    Input (Latin)     │     Predicted (Telugu)      │       Target (Telugu)       │   Match   │\n╞══════════════════════╪═════════════════════════════╪═════════════════════════════╪═══════════╡\n│  \u001b[92mdvepanni          \u001b[0m  │    \u001b[92mద్వీపాన్ని               \u001b[0m     │    \u001b[92mద్వీపాన్ని               \u001b[0m     │     \u001b[92m✓\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[91mbutakapu          \u001b[0m  │   \u001b[91mబుతాపపు                  \u001b[0m    │   \u001b[91mబూటకపు                   \u001b[0m   │     \u001b[91m✗\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[92mudayaniki         \u001b[0m  │   \u001b[92mఉదయానికి                 \u001b[0m    │   \u001b[92mఉదయానికి                 \u001b[0m    │     \u001b[92m✓\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[92mdigindi           \u001b[0m  │    \u001b[92mదిగింది                  \u001b[0m    │    \u001b[92mదిగింది                  \u001b[0m    │     \u001b[92m✓\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[92msamithiloo        \u001b[0m  │   \u001b[92mసమితిలో                  \u001b[0m    │   \u001b[92mసమితిలో                  \u001b[0m    │     \u001b[92m✓\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[91maarohanha         \u001b[0m  │   \u001b[91mఆరోహణా                   \u001b[0m   │  \u001b[91mఆరోహణ                    \u001b[0m   │     \u001b[91m✗\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[91mshelli            \u001b[0m  │   \u001b[91mశెళ్లి                   \u001b[0m    │   \u001b[91mషెల్లీ                   \u001b[0m    │     \u001b[91m✗\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[91mnuditi            \u001b[0m  │   \u001b[91mనుడిటి                   \u001b[0m    │   \u001b[91mనుదిటి                   \u001b[0m    │     \u001b[91m✗\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[91maastreliyaalooni  \u001b[0m  │     \u001b[91mఆస్రీలీయాలిని            \u001b[0m     │     \u001b[91mఆస్ట్రేలియాలోని          \u001b[0m      │     \u001b[91m✗\u001b[0m     │\n├──────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────┤\n│  \u001b[91mbadmeenton        \u001b[0m  │    \u001b[91mబెడ్మెంటన్               \u001b[0m     │     \u001b[91mబ్యాడ్మింటన్             \u001b[0m     │     \u001b[91m✗\u001b[0m     │\n╘══════════════════════╧═════════════════════════════╧═════════════════════════════╧═══════════╛\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
