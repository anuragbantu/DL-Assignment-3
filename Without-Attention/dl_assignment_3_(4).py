# -*- coding: utf-8 -*-
"""dl-assignment-3 (4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1saN-pxblFmaMkPWJ7T-dLVHfTe-4agvg
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# 1. Data Import

Import the data from the TSV files.
"""

import pandas as pd

# Load train and validation datasets from TSV files
train_df = pd.read_csv("/kaggle/input/telugu-lexicon/te.translit.sampled.train.tsv", sep="\t", header=None)
train_df.columns = ["target", "source", "label"]


val_df = pd.read_csv("/kaggle/input/telugu-lexicon/te.translit.sampled.dev.tsv", sep="\t", header=None)
val_df.columns = ["target", "source", "label"]

# Drop any rows where source or target is missing
train_df = train_df.dropna(subset=["source", "target"])
val_df = val_df.dropna(subset=["source", "target"])

# Ensure source and target are strings
train_df["source"] = train_df["source"].astype(str)
train_df["target"] = train_df["target"].astype(str)
val_df["source"] = val_df["source"].astype(str)
val_df["target"] = val_df["target"].astype(str)

# Create pairs for training and validation
pairs = list(zip(train_df["source"], train_df["target"]))
val_pairs = list(zip(val_df["source"], val_df["target"]))

import random

# Sample 10,000 examples for faster experimentation. Otherwise, training takes too long

sample_size = 10000
pairs = random.sample(pairs, sample_size)

import torch
import torch.nn as nn
import torch.optim as optim

"""# 2. Tokenization and Vocabulary Indexing

"""

# Extract unique characters for both source and target languages

SRC_CHARS = set("".join(s for s, _ in pairs))
TRG_CHARS = set("".join(t for _, t in pairs)) | {"<sos>", "<eos>"}# include special tokens

# Create index mappings for characters

src2idx = {ch: i+1 for i, ch in enumerate(sorted(SRC_CHARS))}  # reserve 0 for padding
src2idx["<pad>"] = 0
trg2idx = {ch: i+1 for i, ch in enumerate(sorted(TRG_CHARS))}
trg2idx["<pad>"] = 0

idx2trg = {i: ch for ch, i in trg2idx.items()}

"""# 3. Model Definition: Encoder-Decoder

Define the encoder-decoder structure.
"""

import torch
import torch.nn as nn

# Utility function to select the RNN cell type
def get_rnn_cell(cell_type):
    cell_type = cell_type.upper()
    if cell_type == "GRU":
        return nn.GRU
    elif cell_type == "LSTM":
        return nn.LSTM
    elif cell_type == "RNN":
        return nn.RNN
    else:
        raise ValueError("Unsupported RNN cell type. Use 'RNN', 'GRU', or 'LSTM'.")

# Encoder: Converts input sequences to hidden representations
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, cell_type="GRU", num_layers=1, dropout=0.0):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)
        self.rnn = get_rnn_cell(cell_type)(
            emb_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0.0
        )
    def forward(self, src):
        embedded = self.embedding(src)  # [src_len, batch=1, emb_dim]
        outputs, hidden = self.rnn(embedded)  # hidden: [num_layers, batch, hidden_dim]
        return hidden

# Decoder: Generates output sequences one token at a time
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, cell_type="GRU", num_layers=1, dropout=0.0):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)
        self.rnn = get_rnn_cell(cell_type)(
            emb_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0.0
        )
        self.out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_step, hidden):
        embedded = self.embedding(input_step)
        output, hidden = self.rnn(embedded, hidden)
        output = self.dropout(output.squeeze(0))  # Apply dropout to RNN output
        prediction = self.out(output)
        return prediction, hidden

# Seq2Seq: Full encoder-decoder wrapper
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device, sos_idx):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.sos_idx = sos_idx

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        trg_len = trg.shape[0]
        output_dim = self.decoder.out.out_features
        outputs = torch.zeros(trg_len, 1, output_dim).to(self.device)

        hidden = self.encoder(src)
        hidden = self.adjust_hidden_for_decoder(hidden, self.decoder.rnn.num_layers)

        input_step = torch.tensor([[self.sos_idx]], device=self.device) # Start with <sos>

        for t in range(trg_len):
            output, hidden = self.decoder(input_step, hidden)
            outputs[t] = output
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1).unsqueeze(0) # Get top predicted token
            input_step = trg[t].unsqueeze(0) if teacher_force else top1

        return outputs

    def adjust_hidden_for_decoder(self, hidden, target_layers):
        """
        Adjust the encoder's hidden state to match the number of decoder layers.
        Pads if encoder has fewer layers, trims if more.
        Works for GRU/RNN (tensor) and LSTM (tuple).
        """
        if isinstance(hidden, tuple):  # LSTM
            h, c = hidden
            h = self._match_layers(h, target_layers)
            c = self._match_layers(c, target_layers)
            return (h, c)
        else:  # GRU or RNN
            return self._match_layers(hidden, target_layers)

    def _match_layers(self, state, target_layers):
        """
        Pad or trim the hidden state tensor to match target number of layers.
        """
        current_layers = state.size(0)
        if current_layers == target_layers:
            return state
        elif current_layers < target_layers:
            diff = target_layers - current_layers
            pad = torch.zeros(diff, state.size(1), state.size(2), device=state.device)
            return torch.cat([state, pad], dim=0)
        else:  # current_layers > target_layers
            return state[:target_layers]

#use GPU for processing if available
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Helper function that converts a word into a tensor of indices based on a given vocabulary
# Optionally adds the <eos> token at the end

def tensor_from_word(word, mapping, add_eos=False):
    indices = [mapping[ch] for ch in word]
    if add_eos:
        indices.append(mapping["<eos>"])
    return torch.tensor(indices, dtype=torch.long, device=DEVICE).unsqueeze(1)

"""#  4. Beam Search Decoding

Define the beam search function.
"""

# Decodes a sequence using beam search to find the most probable output string

def beam_search(model, src_tensor, beam_size=3, max_len=30):
    model.eval()
    with torch.no_grad():
        # Encode the input sequence
        hidden = model.encoder(src_tensor)
        hidden = model.adjust_hidden_for_decoder(hidden, model.decoder.rnn.num_layers)


        # Trim LSTM hidden states if necessary
        if isinstance(hidden, tuple):
            h, c = hidden
            hidden = (
                h[:model.decoder.rnn.num_layers],
                c[:model.decoder.rnn.num_layers],
            )
        else:
            hidden = hidden[:model.decoder.rnn.num_layers]

        # Initialize beam with sequence starting with <sos> token
        sequences = [([model.sos_idx], 0.0, hidden)]

        for _ in range(max_len):
            all_candidates = []
            for seq, score, h in sequences:
                input_step = torch.tensor([[seq[-1]]], device=model.device)
                output, h_new = model.decoder(input_step, h) # Run decoder
                probs = torch.log_softmax(output, dim=1) # Log-probabilities
                topk = torch.topk(probs, beam_size) # Top-k predictions

                # Expand each current sequence with top-k new tokens
                for i in range(beam_size):
                    token = topk.indices[0][i].item()
                    token_score = topk.values[0][i].item()
                    new_seq = seq + [token]
                    all_candidates.append((new_seq, score + token_score, h_new))

            # Keep top beam_size sequences with highest scores
            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]

            # If all sequences have ended with <eos>, stop early
            if all(seq[-1] == trg2idx["<eos>"] for seq, _, _ in sequences):
                break

        # Return best sequence (excluding <sos> and <eos> tokens)
        best_seq = sequences[0][0][1:]  # remove <sos>
        return "".join([idx2trg[i] for i in best_seq if i != trg2idx["<eos>"]])

"""#  5. Training and Evaluation Utilities

Define the training function and run the wandb sweep.
"""

#install wandb and login

!pip install -q wandb
import wandb

wandb.login(key='af7d7cf29d8954a13afb06c7a0d0c196c36ac51b')

#set all the hyperparameter values to test in the sweep

sweep_config = {
    "method": "bayes",
    "metric": {"name": "val_acc", "goal": "maximize"},
    "parameters": {
        "emb_dim": {"values": [16,32,64,256]},
        "hidden_dim": {"values": [16,32,64,256]},
        "cell_type": {"values": ["RNN","GRU","LSTM"]},
        "enc_layers": {"values": [1,2,3]},
        "dec_layers": {"values": [1, 2, 3]},
        "dropout": {"values": [0,0.2, 0.3]},
        "beam_size": {"values": [1, 3, 2]},
        "lr": {"values": [0.001, 0.0005]},
        "teacher_forcing_ratio": {"values": [0.3, 0.5, 0.7, 1.0]}

    }
}

# This section includes the model training loop, validation metrics computation,
# and logging to Weights & Biases for hyperparameter tuning and experiment tracking.


def train_model(config=None):
    # Initialize a Weights & Biases run with the given configuration
    with wandb.init(config=config) as run:
        config = wandb.config

        # Set a descriptive run name for easier tracking in WandB dashboard
        run.name = f"emb{config.emb_dim}_hid{config.hidden_dim}_{config.cell_type}_enc{config.enc_layers}_dec{config.dec_layers}_drop{int(config.dropout*100)}_beam{config.beam_size}_lr{config.lr}"

        # Initialize encoder and decoder models
        encoder = Encoder(len(src2idx), config.emb_dim, config.hidden_dim,
                          config.cell_type, config.enc_layers, config.dropout)
        decoder = Decoder(len(trg2idx), config.emb_dim, config.hidden_dim,
                          config.cell_type, config.dec_layers, config.dropout)
        model = Seq2Seq(encoder, decoder, DEVICE, sos_idx=trg2idx["<sos>"]).to(DEVICE)

        # Define optimizer and loss criterion
        optimizer = optim.Adam(model.parameters(), lr=config.lr)
        criterion = nn.CrossEntropyLoss(ignore_index=trg2idx["<pad>"])
        beam_size = config.get("beam_size", 3)

        # Training loop across multiple epochs
        for epoch in range(1, 6):
            model.train()
            total_loss = 0
            train_correct = 0
            train_total = 0

            # === Training Phase ===
            for src_word, trg_word in pairs:
                # Convert source and target words to tensors
                src_tensor = tensor_from_word(src_word, src2idx)
                trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)

                # Zero the parameter gradients
                optimizer.zero_grad()

                # Forward pass with optional teacher forcing
                output = model(src_tensor, trg_tensor, teacher_forcing_ratio=config.get("teacher_forcing_ratio", 0.5))
                loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))

                # Backpropagation and parameter update
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

                # Character-level training accuracy
                pred_tokens = output.argmax(-1).view(-1)
                true_tokens = trg_tensor.view(-1)
                mask = true_tokens != trg2idx["<pad>"]
                correct = (pred_tokens == true_tokens) & mask
                train_correct += correct.sum().item()
                train_total += mask.sum().item()

            # === Validation Phase ===
            model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            exact_match_count = 0

            with torch.no_grad():
                for src_word, trg_word in val_pairs:
                    src_tensor = tensor_from_word(src_word, src2idx)
                    trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)

                    # Predict without teacher forcing
                    output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0.0)
                    loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))
                    val_loss += loss.item()

                    # Character-level validation accuracy
                    pred_tokens = output.argmax(-1).view(-1)
                    true_tokens = trg_tensor.view(-1)
                    mask = true_tokens != trg2idx["<pad>"]
                    correct = (pred_tokens == true_tokens) & mask
                    val_correct += correct.sum().item()
                    val_total += mask.sum().item()

                    # Sequence-level exact match using beam search
                    pred_str = beam_search(model, src_tensor, beam_size=beam_size)
                    if pred_str == trg_word:
                        exact_match_count += 1

            # Compute metrics
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            exact_match = exact_match_count / len(val_pairs)

            # Print metrics for the current epoch
            print(f"Epoch {epoch:2d} | "
                  f"Train Loss: {total_loss / len(pairs):.4f} | "
                  f"Train Acc: {train_acc:.4f} | "
                  f"Val Loss: {val_loss / len(val_pairs):.4f} | "
                  f"Val Acc: {val_acc:.4f} | "
                  f"Val Exact Match: {exact_match:.4f} | "
                  f"Beam Size: {beam_size}")


            # Log metrics to WandB for visualization and tracking
            wandb.log({
                "epoch": epoch,
                "train_loss": total_loss / len(pairs),
                "val_loss": val_loss / len(val_pairs),
                "train_accuracy": train_correct / train_total,
                "val_accuracy": val_correct / val_total,
                "val_exact_match": exact_match_count / len(val_pairs),
                "beam_size": beam_size
            })

#run the wandb sweep


sweep_id = wandb.sweep(sweep_config, project="transliteration-sweep")
wandb.agent(sweep_id, function=train_model, count=10)
wandb.finish()

"""# 6. Best model Evaluation

Evaluate the best performing model on the test dataset. The following configuration was observed to be the best performing model from the wandb sweep analysis.
"""

# Define the best configuration obtained from hyperparameter tuning

best_config = {
    "emb_dim": 32,
    "hidden_dim": 256,
    "cell_type": "LSTM",
    "enc_layers": 3,
    "dec_layers": 2,
    "dropout": 0,
    "beam_size": 3,
    "lr": 0.001}

"""Defining a new train function (with similar structure as the training function before) which returns the best model (with trained weights) as output so that we can test the performance on test data."""

# Train the model using the best configuration

def best_train_model(config=None):
    with wandb.init(config=config):
        config = wandb.config

        encoder = Encoder(len(src2idx), config.emb_dim, config.hidden_dim,
                          config.cell_type, config.enc_layers, config.dropout)
        decoder = Decoder(len(trg2idx), config.emb_dim, config.hidden_dim,
                          config.cell_type, config.dec_layers, config.dropout)
        model = Seq2Seq(encoder, decoder, DEVICE, sos_idx=trg2idx["<sos>"]).to(DEVICE)

        optimizer = optim.Adam(model.parameters(), lr=config.lr)
        criterion = nn.CrossEntropyLoss(ignore_index=trg2idx["<pad>"])
        beam_size = config.get("beam_size", 3)

        #train the best model for 10 epochs
        for epoch in range(1, 11):
            model.train()
            total_loss = 0
            train_correct = 0
            train_total = 0

            for src_word, trg_word in pairs:
                src_tensor = tensor_from_word(src_word, src2idx)
                trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)

                optimizer.zero_grad()
                output = model(src_tensor, trg_tensor, teacher_forcing_ratio=config.get("teacher_forcing_ratio", 0.5))
                loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

                pred_tokens = output.argmax(-1).view(-1)
                true_tokens = trg_tensor.view(-1)
                mask = true_tokens != trg2idx["<pad>"]
                correct = (pred_tokens == true_tokens) & mask
                train_correct += correct.sum().item()
                train_total += mask.sum().item()

            model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            exact_match_count = 0

            with torch.no_grad():
                for src_word, trg_word in val_pairs:
                    src_tensor = tensor_from_word(src_word, src2idx)
                    trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)

                    output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0.0)
                    loss = criterion(output.view(-1, output.size(-1)), trg_tensor.view(-1))
                    val_loss += loss.item()

                    pred_tokens = output.argmax(-1).view(-1)
                    true_tokens = trg_tensor.view(-1)
                    mask = true_tokens != trg2idx["<pad>"]
                    correct = (pred_tokens == true_tokens) & mask
                    val_correct += correct.sum().item()
                    val_total += mask.sum().item()

                    # Beam search for exact match accuracy
                    pred_str = beam_search(model, src_tensor, beam_size=beam_size)
                    if pred_str == trg_word:
                        exact_match_count += 1

            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            exact_match = exact_match_count / len(val_pairs)

            print(f"Epoch {epoch:2d} | "
                  f"Train Loss: {total_loss / len(pairs):.4f} | "
                  f"Train Acc: {train_acc:.4f} | "
                  f"Val Loss: {val_loss / len(val_pairs):.4f} | "
                  f"Val Acc: {val_acc:.4f} | "
                  f"Val Exact Match: {exact_match:.4f} | "
                  f"Beam Size: {beam_size}")

            wandb.log({
                "epoch": epoch,
                "train_loss": total_loss / len(pairs),
                "val_loss": val_loss / len(val_pairs),
                "train_accuracy": train_acc,
                "val_accuracy": val_acc,
                "val_exact_match": exact_match,
                "beam_size": beam_size
            })

        return model

#train the best model

model = best_train_model(config=best_config)

#load the test data
test_df = pd.read_csv("/kaggle/input/telugu-lexicon/te.translit.sampled.test.tsv", sep="\t", header=None)
test_df.columns = ["target", "source", "label"]  # adjust if only 2 columns
test_df = test_df.dropna(subset=["source", "target"])
test_df["source"] = test_df["source"].astype(str)
test_df["target"] = test_df["target"].astype(str)
test_pairs = list(zip(test_df["source"], test_df["target"]))

print(len(test_pairs))

# Evaluate model performance on test set

model.eval()
test_correct = 0
test_total = 0
exact_match_count = 0
beam_size = best_config["beam_size"]

with torch.no_grad():
    for src_word, trg_word in test_pairs:
        src_tensor = tensor_from_word(src_word, src2idx)
        trg_tensor = tensor_from_word(trg_word, trg2idx, add_eos=True)

        output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0.0)

        # Character-level accuracy
        pred_tokens = output.argmax(-1).view(-1)
        true_tokens = trg_tensor.view(-1)
        mask = true_tokens != trg2idx["<pad>"]
        correct = (pred_tokens == true_tokens) & mask
        test_correct += correct.sum().item()
        test_total += mask.sum().item()

        # Word-level exact match via beam search
        pred_str = beam_search(model, src_tensor, beam_size=beam_size)
        if pred_str == trg_word:
            exact_match_count += 1

test_char_acc = test_correct / test_total
test_exact_match = exact_match_count / len(test_pairs)

print(f"Test Char Accuracy: {test_char_acc:.4f}")
print(f"Test Exact Match:  {test_exact_match:.4f}")

"""Saving all the prediction on test data to a TSV file in folder predictions_vanilla."""

import csv, os, pandas as pd, torch

#  Run model on the whole test set once
model.eval()
beam = best_config["beam_size"]
records = []                         # rows for the output file

with torch.no_grad():
    for latin, true in test_pairs:
        pred = beam_search(model,
                           tensor_from_word(latin, src2idx),
                           beam_size=beam)
        records.append({"latin": latin,
                        "true": true,
                        "pred": pred})

# Save as TSV
out_df = pd.DataFrame(records)
os.makedirs("predictions_vanilla", exist_ok=True)          # local folder
out_path = "predictions_vanilla/test_predictions.tsv"
out_df.to_csv(out_path, sep="\t", index=False)
print(f"Saved {len(out_df)} rows ➜ {out_path}")

"""Visualizing the predictions of the best model."""

# Display random predictions in a color-coded table

from tabulate import tabulate
import random

def display_predictions(model, test_pairs, src2idx, beam_size=3, max_samples=10):
    model.eval()
    table = []

    # ANSI escape codes for red and green coloring in terminal output
    RED = "\033[91m"
    GREEN = "\033[92m"
    RESET = "\033[0m"

    col_widths = {
        "input": 18,
        "pred": 25,
        "target": 25,
        "match": 7
    }

    with torch.no_grad():
        # Randomly sample a few test examples
        sampled_pairs = random.sample(test_pairs, k=min(max_samples, len(test_pairs)))
        for src_word, trg_word in sampled_pairs:
            # Convert input to tensor and run beam search
            src_tensor = tensor_from_word(src_word, src2idx)
            pred_str = beam_search(model, src_tensor, beam_size=beam_size)
            is_match = pred_str == trg_word

            # Format each column entry with appropriate alignment
            row_data = [
                f"{src_word:<{col_widths['input']}}",
                f"{pred_str:<{col_widths['pred']}}",
                f"{trg_word:<{col_widths['target']}}",
                "✓" if is_match else "✗"
            ]

            # Apply color to entire row based on correctness
            color = GREEN if is_match else RED
            colored_row = [f"{color}{cell}{RESET}" for cell in row_data]

            table.append(colored_row)

    # Create headers with center alignment
    headers = [
        f"{'Input (Latin)':^{col_widths['input']}}",
        f"{'Predicted (Telugu)':^{col_widths['pred']}}",
        f"{'Target (Telugu)':^{col_widths['target']}}",
        f"{'Match':^{col_widths['match']}}"
    ]

    # Display the results as a formatted table
    print(tabulate(table, headers=headers, tablefmt="fancy_grid", stralign="center"))

display_predictions(model, test_pairs,src2idx, beam_size=best_config["beam_size"], max_samples=10)

